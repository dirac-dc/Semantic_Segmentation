{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution()\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "%load_ext pycodestyle_magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FEAT_DIR = './data/train/images'\n",
    "TRAIN_LABELS_DIR = './data/train/masks'\n",
    "VGG_PATH = '/home/paperspace/kaggle/Semantic_Segmentation/vgg16/saved_model'\n",
    "NUM_CLASSES = 1\n",
    "LRATE = 0.001\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "IMAGE_SHAPE = (101, 101, 3)\n",
    "LABEL_SHAPE = (101, 101)\n",
    "INPUT_SHAPE = (104, 104, 3)\n",
    "OUTPUT_SHAPE = (104, 104)\n",
    "# input different to the image dimensions due to a hack to make the layers symmetric when\n",
    "# deconvoluting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%pycodestyle\n",
    "\n",
    "class data_handling:\n",
    "    \n",
    "    def __init__(self, train_feat_path, train_label_path):\n",
    "        self.train_feat_path = train_feat_path\n",
    "        self.train_label_path = train_label_path\n",
    "        \n",
    "        self.train_feat_data, \\\n",
    "        self.val_feat_data, \\\n",
    "        self.test_feat_data = \\\n",
    "        self.split_data(self.create_array(train_feat_path))\n",
    "        self.train_label_data, \\\n",
    "        self.val_label_data, \\\n",
    "        self.test_label_data =\\\n",
    "        self.split_data(\n",
    "            self.create_array(train_label_path, mode='label')/65535.0)\n",
    "    \n",
    "    def gen_batch_function(self, dataset='train',\n",
    "                           bs=BATCH_SIZE, num_batches=None):\n",
    "        \n",
    "        if dataset == 'train':\n",
    "            feat = self.train_feat_data\n",
    "            labels = self.train_label_data\n",
    "            \n",
    "        elif dataset == 'test':\n",
    "            feat = self.test_feat_data\n",
    "            labels = self.test_label_data\n",
    "            \n",
    "        if num_batches is None:\n",
    "            stop_iter = len(feat)//bs + 1\n",
    "        else:\n",
    "            stop_iter = num_batches\n",
    "        \n",
    "        batch = 0\n",
    "        \n",
    "        for i in range(stop_iter):\n",
    "            if batch != len(feat)//bs:\n",
    "                \n",
    "                st = batch*bs; end = (batch+1)*bs;\n",
    "                \n",
    "                yield (feat[st:end,:].astype('float32')\\\n",
    "                - self.get_mean()), \\\n",
    "                labels[st:end,:].astype('float32')\n",
    "                \n",
    "                batch += 1\n",
    "            else:\n",
    "                yield feat[batch*bs:(len(feat)),:].astype('float32')\\\n",
    "                 - self.get_mean(), \\\n",
    "                labels[batch*bs:(len(feat)),:].astype('float32')\n",
    "\n",
    "    def create_array(self, path, mode = 'train'):\n",
    "        files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "        \n",
    "        if mode=='train':\n",
    "            data = np.zeros((len(files), *INPUT_SHAPE)).astype('int')\n",
    "        elif mode=='label':\n",
    "            data = np.zeros((len(files), *OUTPUT_SHAPE)).astype('int')\n",
    "            \n",
    "        for i in range(len(files)):\n",
    "            if mode == 'train':\n",
    "                data[i,\n",
    "                     :IMAGE_SHAPE[0],\n",
    "                     :IMAGE_SHAPE[1],\n",
    "                     :] = np.array(Image.open(path + '/'+ files[i])) \n",
    "            elif mode == 'label':\n",
    "                data[i,\n",
    "                     :IMAGE_SHAPE[0],\n",
    "                     :IMAGE_SHAPE[1]\n",
    "                    ] = np.array(Image.open(path + '/'+ files[i]))                \n",
    "        return data\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_mean():\n",
    "        x = np.zeros((1,1,1,3))\n",
    "        x[0,0,0,:]= np.array([120.346, 120.346, 120.346])\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_std():\n",
    "        return 27.60\n",
    "    \n",
    "    @staticmethod\n",
    "    def shuffle(data):\n",
    "        ind = np.random.choice(len(data),len(data), replace=False)\n",
    "        return data[ind]\n",
    "    \n",
    "    @staticmethod\n",
    "    def split_data(data, val_split = 0.05, split = 0.8):\n",
    "        train_end = int(len(data)*(split - val_split))\n",
    "        val_end = int(len(data)*split)\n",
    "        train_feat = data[:train_end]\n",
    "        val_data = data[train_end:val_end]\n",
    "        test_data = data[val_end:]\n",
    "        return train_feat, val_data, test_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt = data_handling(TRAIN_FEAT_DIR, TRAIN_LABELS_DIR)\n",
    "\n",
    "# def s_data(data, val_split = 0.05, split = 0.8):\n",
    "#     train_end = len(data)*int(split - val_split)\n",
    "#     val_end = len(data)*int(split)\n",
    "#     train_feat = data[:train_end]\n",
    "#     val_data = data[train_end:val_end]\n",
    "#     test_data = data[val_end:]\n",
    "#     return train_feat, val_data, test_data\n",
    "    \n",
    "\n",
    "# s_data(dt.create_array(TRAIN_FEAT_DIR))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### run code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vgg(sess, vgg_path = VGG_PATH):\n",
    "  \n",
    "   # load the model and weights\n",
    "    model = tf.saved_model.loader.load(sess, ['vgg16'], vgg_path)\n",
    "\n",
    "   # Get Tensors to be returned from graph\n",
    "    graph = tf.get_default_graph()\n",
    "    image_input = graph.get_tensor_by_name('image_input:0')\n",
    "   \n",
    "    #keep_prob = graph.get_tensor_by_name('keep_prob:0')\n",
    "    layer3 = graph.get_tensor_by_name('conv2_1:0')\n",
    "    layer4 = graph.get_tensor_by_name('pool2:0')\n",
    "    layer7 = graph.get_tensor_by_name('pool3:0')\n",
    "\n",
    "    return image_input, layer3, layer4, layer7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess = tf.Session()\n",
    "# img, l3, _, _ = load_vgg(sess)\n",
    "# print(img.get_shape(), l3.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layers(vgg_layer3_out, vgg_layer4_out, vgg_layer7_out, num_classes = NUM_CLASSES):\n",
    "   \n",
    "    # Use a shorter variable name for simplicity\n",
    "    layer3, layer4, layer7 = vgg_layer3_out, vgg_layer4_out, vgg_layer7_out\n",
    "\n",
    "    # Apply 1x1 convolution in place of fully connected layer\n",
    "    fcn8 = tf.layers.conv2d(layer7, filters=num_classes, kernel_size=1, name=\"fcn8\")\n",
    "\n",
    "    # Upsample fcn8 with size depth=(4096?) to match size of layer 4 so that we can add skip connection with 4th layer\n",
    "    fcn9 = tf.layers.conv2d_transpose(fcn8, filters=layer4.get_shape().as_list()[-1],\n",
    "    kernel_size=4, strides=(2, 2), padding='SAME', name=\"fcn9\")\n",
    "\n",
    "    # Add a skip connection between current final layer fcn8 and 4th layer\n",
    "    fcn9_skip_connected = tf.add(fcn9, layer4, name=\"fcn9_plus_vgg_layer4\")\n",
    "\n",
    "    # Upsample again\n",
    "    fcn10 = tf.layers.conv2d_transpose(fcn9_skip_connected, filters=layer3.get_shape().as_list()[-1],\n",
    "    kernel_size=4, strides=(2, 2), padding='SAME', name=\"fcn10_conv2d\")\n",
    "\n",
    "    # Add skip connection\n",
    "    fcn10_skip_connected = tf.add(fcn10, layer3, name=\"fcn10_plus_vgg_layer3\")\n",
    "    \n",
    "    # Upsample again\n",
    "    fcn11 = tf.layers.conv2d_transpose(fcn10_skip_connected, filters=NUM_CLASSES,\n",
    "    kernel_size=4, strides=(2, 2), padding='SAME', name=\"fcn11\")\n",
    "\n",
    "    return fcn11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes=NUM_CLASSES\n",
    "\n",
    "# d_proc = data_handling(TRAIN_FEAT_DIR,labels_DIR)\n",
    "\n",
    "# # A function to get batches\n",
    "# get_batches_fn = d_proc.gen_batch_function\n",
    "\n",
    "# with tf.Session() as session:\n",
    "    \n",
    "#     correct_label = tf.placeholder(tf.float32, [None, *OUTPUT_SHAPE], name='correct_label')\n",
    "\n",
    "#     image_input, layer3, layer4, layer7 = load_vgg(session)\n",
    "\n",
    "#     model_output = layers(layer3, layer4, layer7, num_classes = 1)\n",
    "\n",
    "#     logits = tf.reshape(model_output, (-1, num_classes), name=\"fcn_logits\")\n",
    "    \n",
    "#     correct_label_reshaped = tf.reshape(correct_label, (-1, num_classes))\n",
    "\n",
    "#     cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=correct_label_reshaped[:])\n",
    "\n",
    "#     loss_op = tf.reduce_mean(cross_entropy, name=\"fcn_loss\")\n",
    "\n",
    "#     # The model implements this operation to find the weights/parameters that would yield correct pixel labels\n",
    "#     train_op = tf.train.AdamOptimizer(learning_rate=LRATE).minimize(loss_op, name=\"fcn_train_op\")\n",
    "\n",
    "\n",
    "#     # Initialize all variables\n",
    "#     session.run(tf.global_variables_initializer())\n",
    "#     session.run(tf.local_variables_initializer())\n",
    "\n",
    "#     print(\"Model build successful, starting training\")\n",
    "    \n",
    "#     # Train the neural network\n",
    "#     for X_batch, gt_batch in get_batches_fn(num_batches=1):\n",
    "#         ce, loss, _, pre_logit, true_logit = \\\n",
    "#         session.run([cross_entropy, loss_op, train_op, logits, correct_label_reshaped],\n",
    "#         feed_dict={image_input: X_batch,\n",
    "#                  correct_label: gt_batch})\n",
    "    \n",
    "    \n",
    "#     # Run the model with the test images and save each painted output image (roads painted green)\n",
    "#     #helper.save_inference_samples(runs_dir, data_dir, session, image_shape, logits, image_input)\n",
    "\n",
    "#     print(ce, loss, pre_logit, true_logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### full run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(nn_last_layer, correct_label, learning_rate = LRATE, num_classes = NUM_CLASSES):\n",
    "  \n",
    "  # Reshape 4D tensors to 2D, each row represents a pixel, each column a class\n",
    "    logits = tf.reshape(nn_last_layer, (-1, num_classes), name=\"fcn_logits\")\n",
    "    correct_label_reshaped = tf.reshape(correct_label, (-1, num_classes))\n",
    "\n",
    "    # Calculate distance from actual labels using cross entropy\n",
    "    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=correct_label_reshaped[:])\n",
    "    # Take mean for total loss\n",
    "    loss_op = tf.reduce_mean(cross_entropy, name=\"fcn_loss\")\n",
    "\n",
    "    # The model implements this operation to find the weights/parameters that would yield correct pixel labels\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss_op, name=\"fcn_train_op\")\n",
    "\n",
    "    return logits, train_op, loss_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(sess, epochs, batch_size, data_handler, train_op,\n",
    "             cross_entropy_loss, input_image,\n",
    "             correct_label):\n",
    "    \n",
    "    output_path = \"./Train\"\n",
    "    train_summary_writer = tf.summary.FileWriter(output_path)\n",
    "    \n",
    "    train_summary=tf.Summary()\n",
    "    val_summary=tf.Summary()\n",
    "    \n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    \n",
    "    #keep_prob_value = 0.5\n",
    "    #learning_rate_value = LRATE\n",
    "    total_train_loss = 0\n",
    "    total_val_loss = 0\n",
    "    step = 0 \n",
    "    for epoch in range(epochs):\n",
    "      # Create function to get batches\n",
    "        total_train_loss = 0\n",
    "        total_val__loss = 0\n",
    "        for X_batch, gt_batch in data_handler.gen_batch_function(bs = batch_size):\n",
    "            step += 1\n",
    "            loss, _ = sess.run([cross_entropy_loss, train_op],\n",
    "            feed_dict={input_image: X_batch,\n",
    "                     correct_label: gt_batch})\n",
    "            val_loss = sess.run([cross_entropy_loss],\n",
    "            feed_dict={input_image: data_handler.val_feat_data,\n",
    "                     correct_label: data_handler.val_label_data})\n",
    "            #print(loss, np.shape(loss))\n",
    "            #print(val_loss, np.shape(val_loss))\n",
    "            train_summary.value.add(tag='train_loss', simple_value = loss)\n",
    "            val_summary.value.add(tag='val_loss', simple_value = val_loss[0])\n",
    "            train_summary_writer.add_summary(train_summary, step)\n",
    "            train_summary_writer.add_summary(val_summary, step)\n",
    "            \n",
    "            # train_summary_writer.flush()\n",
    "            total_train_loss += loss;\n",
    "            total_val_loss += val_loss[0]\n",
    "        print(\"EPOCH {} ...\".format(epoch + 1))\n",
    "        print(\"Loss = {:.3f};  Val_loss = {:.3f}\".format(total_train_loss, total_val_loss))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "  \n",
    "  d_proc = data_handling(TRAIN_FEAT_DIR, TRAIN_LABELS_DIR)\n",
    "  \n",
    "  with tf.Session() as session:\n",
    "    \n",
    "    correct_label = tf.placeholder(tf.float32, [None, *OUTPUT_SHAPE], name='correct_label')\n",
    "    \n",
    "    # Returns the three layers, keep probability and input layer from the vgg architecture\n",
    "    image_input, layer3, layer4, layer7 = load_vgg(session)\n",
    "    #print(layer3.get_shape())\n",
    "    # The resulting network architecture from adding a decoder on top of the given vgg model\n",
    "    \n",
    "    model_output = layers(layer3, layer4, layer7, num_classes = 1)\n",
    "\n",
    "    # Returns the output logits, training operation and cost operation to be used\n",
    "    # - logits: each row represents a pixel, each column a class\n",
    "    # - train_op: function used to get the right parameters to the model to correctly label the pixels\n",
    "    # - cross_entropy_loss: function outputting the cost which we are minimizing, lower cost should yield higher accuracy\n",
    "    logits, train_op, cross_entropy_loss = optimize(model_output, correct_label)\n",
    "    \n",
    "#     # Initialize all variables\n",
    "#     session.run(tf.global_variables_initializer())\n",
    "#     session.run(tf.local_variables_initializer())\n",
    "\n",
    "    print(\"Model build successful, starting training\")\n",
    "\n",
    "    # Train the neural network\n",
    "    train_nn(session, EPOCHS, BATCH_SIZE, d_proc, \n",
    "             train_op, cross_entropy_loss, image_input,\n",
    "             correct_label)\n",
    "    \n",
    "    # Run the model with the test images and save each painted output image (roads painted green)\n",
    "    #helper.save_inference_samples(runs_dir, data_dir, session, image_shape, logits, image_input)\n",
    "    \n",
    "    print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/paperspace/kaggle/Semantic_Segmentation/vgg16/saved_model/variables/variables\n",
      "(?, 52, 52, 128)\n",
      "Model build successful, starting training\n",
      "19.722923 ()\n",
      "[584.34894] (1,)\n",
      "223.5613 ()\n",
      "[32.249283] (1,)\n",
      "12.17002 ()\n",
      "[43.081917] (1,)\n",
      "15.265841 ()\n",
      "[36.001305] (1,)\n",
      "17.007084 ()\n",
      "[27.30029] (1,)\n",
      "7.832636 ()\n",
      "[34.286205] (1,)\n",
      "7.7442474 ()\n",
      "[32.581745] (1,)\n",
      "7.5803742 ()\n",
      "[24.622812] (1,)\n",
      "5.9486237 ()\n",
      "[15.942922] (1,)\n",
      "3.6412022 ()\n",
      "[14.443981] (1,)\n",
      "4.1749034 ()\n",
      "[13.860594] (1,)\n",
      "4.7567477 ()\n",
      "[10.1611185] (1,)\n",
      "3.1819313 ()\n",
      "[7.143816] (1,)\n",
      "1.9935011 ()\n",
      "[6.023562] (1,)\n",
      "2.0258787 ()\n",
      "[5.2743] (1,)\n",
      "1.4795595 ()\n",
      "[4.189487] (1,)\n",
      "1.7474203 ()\n",
      "[4.07277] (1,)\n",
      "1.1782188 ()\n",
      "[6.545718] (1,)\n",
      "1.3525051 ()\n",
      "[8.2095175] (1,)\n",
      "1.4126292 ()\n",
      "[7.6780887] (1,)\n",
      "1.243097 ()\n",
      "[5.7137027] (1,)\n",
      "1.1100152 ()\n",
      "[3.7101665] (1,)\n",
      "1.1886724 ()\n",
      "[2.633136] (1,)\n",
      "0.94410735 ()\n",
      "[2.1665473] (1,)\n",
      "1.005461 ()\n",
      "[1.9762927] (1,)\n",
      "0.8145219 ()\n",
      "[1.9907472] (1,)\n",
      "0.8301694 ()\n",
      "[2.245758] (1,)\n",
      "0.7524047 ()\n",
      "[2.654189] (1,)\n",
      "0.7054032 ()\n",
      "[2.7529857] (1,)\n",
      "0.79651636 ()\n",
      "[2.5348375] (1,)\n",
      "0.99744165 ()\n",
      "[2.2135] (1,)\n",
      "0.6396978 ()\n",
      "[1.8071996] (1,)\n",
      "0.7325804 ()\n",
      "[1.5229988] (1,)\n",
      "0.72963184 ()\n",
      "[1.379761] (1,)\n",
      "0.599017 ()\n",
      "[1.313624] (1,)\n",
      "0.72069657 ()\n",
      "[1.2791321] (1,)\n",
      "0.58268946 ()\n",
      "[1.2491045] (1,)\n",
      "0.75021726 ()\n",
      "[1.2026324] (1,)\n",
      "0.6101079 ()\n",
      "[1.1689029] (1,)\n",
      "0.62221944 ()\n",
      "[1.1500155] (1,)\n",
      "0.590106 ()\n",
      "[1.1527114] (1,)\n",
      "0.66283464 ()\n",
      "[1.1924797] (1,)\n",
      "0.6632868 ()\n",
      "[1.2379358] (1,)\n",
      "0.6247462 ()\n",
      "[1.2947377] (1,)\n",
      "0.6023305 ()\n",
      "[1.331575] (1,)\n",
      "0.5394269 ()\n",
      "[1.3025658] (1,)\n",
      "0.5715583 ()\n",
      "[1.2467736] (1,)\n",
      "EPOCH 1 ...\n",
      "Loss = 364.406;  Val_loss = 969.442\n",
      "\n",
      "0.5167476 ()\n",
      "[1.1594087] (1,)\n",
      "0.5492114 ()\n",
      "[1.054698] (1,)\n",
      "0.57389355 ()\n",
      "[0.98762345] (1,)\n",
      "0.47968915 ()\n",
      "[0.9317687] (1,)\n",
      "0.6325946 ()\n",
      "[0.9180563] (1,)\n",
      "0.5808687 ()\n",
      "[0.91855663] (1,)\n",
      "0.559245 ()\n",
      "[0.9318825] (1,)\n",
      "0.55227834 ()\n",
      "[0.9643124] (1,)\n",
      "0.48071218 ()\n",
      "[1.003523] (1,)\n",
      "0.5073828 ()\n",
      "[1.013978] (1,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ec9775ede022>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-b0e1d4922ff6>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     train_nn(session, EPOCHS, BATCH_SIZE, d_proc, \n\u001b[1;32m     30\u001b[0m              \u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m              correct_label)\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Run the model with the test images and save each painted output image (roads painted green)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-fbba8b70f161>\u001b[0m in \u001b[0;36mtrain_nn\u001b[0;34m(sess, epochs, batch_size, data_handler, train_op, cross_entropy_loss, input_image, correct_label)\u001b[0m\n\u001b[1;32m     29\u001b[0m             val_loss = sess.run([cross_entropy_loss],\n\u001b[1;32m     30\u001b[0m             feed_dict={input_image: data_handler.val_feat_data,\n\u001b[0;32m---> 31\u001b[0;31m                      correct_label: data_handler.val_label_data})\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
